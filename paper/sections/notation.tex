\section{Preliminaries and Notation}
\label{sec:notation}

\begin{note}
  Given two vector spaces $V_1, V_2$ we write $V_1 + V_2$ to 
  denote $\mspan{V_1 \cup V_2}$
\end{note}


\begin{defn}
  A \textit{weighted automaton} over a field $\K$ and an alphabet $A$ is a triple 
  $(X,o,t)$ such that $X$ is a finite set of states, 
  $t = \left\lbrace t_a : X \to \K^X\right\rbrace_{a \in A}$
  is a set of transition functions indexed over the symbols of the alphabet $A$ and 
  $o : X  \to \K$ is the output function. 
  The transition functions will be represented as $X \times X$ matrices.
  $A^*$ is the set of all words over $A$, more precisely the free monoid
  with string concatenation as the monoid operation and the empty word $\epsilon$ 
  as the identity element. We denote with $aw$ the
  concatenation of a symbol $a$ to the word $w \in A^*$.
  A weighted language is a function $\psi: A^* \to \K$.
  A function mapping each state vector into its 
  accepted language, $\sqmap{\cdot}: \K^X \to \K^{A^*}$ is defined as follows for 
  every weighted automaton:

  \begin{equation*}
    \begin{aligned}
      \forall v \in \K^X, a \in A, w \in A^* \quad \quad
      \sqmap{v}(\epsilon) = o(v) \quad \quad
      \sqmap{v}(aw) = \sqmap{t_a(v)}(w)  
    \end{aligned}
  \end{equation*}
\end{defn}

Two vectors $v_1, v_2 \in \K^{X\times 1}$ are called weighted language equivalent, 
denoted with  $v_1 \sim_l v_2 $ if and only if 
$ \sqmap {v_1} = \sqmap{v_2}$. One can extend the notion of language 
equivalence to states rather than for vectors by assigning 
to each state $x \in X$ the corresponding  unit vector 
$e_x \in \K^X$. When given an initial state $i$ for a weighted automaton, 
the language  of the automaton can be defined as $\sqmap{i}$.


\begin{defn}
  A binary relation $R \subseteq X \times Y$ between two sets $X, Y$ is a 
  subset of the 
  cartesian product of the sets. A relation is called \textit{homogeneous} 
  or an \textit
  {endorelation} if it is a binary relation over $X$ and itself: 
  $R \subseteq X \times
   X$. 
  In such case, it is simply called a binary relation over $X$.
  An \textit{equivalence relation} is a binary relation that is reflexive, 
  symmetric and
  transitive. 
\end{defn}

\begin{defn}
  The congruence closure $c(R)$ of a relation R is the smallest congruence relation 
  $R'$ such that $R \subseteq R'$ 
\end{defn}

An equivalence relation which is compatible with all the operations of
the algebraic structure on which it is defined on, is called a 
\textit{congruence relation}. Compatibility with the algebraic structure operations
means that algebraic operations applied on equivalent elements will still
yield equivalent elements. 



%===================================================================================

We omit the coalgebraic definition for \textit{linear weighted automata} seen in 
\cite{BONCHI201277} and give a more intuitive definition, which fits our  
implementation when $\K = \R$.
In this implementation, we focus only on weighted automata defined over 
the field of real numbers $\R$. 

\begin{defn}
  A \textit{linear weighted automaton} (in short, LWA) over the field $\K$ and 
  an alphabet $A$
  is a triple  $L = (V, o, \left\lbrace t_a \right\rbrace_{a \in A})$ 
  where $V$ is a vector space representing the state space, 
  $o: V \to \K$ is a linear map associating to each state its output weight,
  and $t = \left\lbrace t_a = V \times V \right\rbrace_{a \in A}$ is
  the set of transition functions, represented with liner maps 
  that for each input $a \in A$ associate the next state, in this case a vector
  in $V$.
  As in \cite{boreale2009weighted}, we have that $\dim{(L)} = \dim{(V)}$.
\end{defn}

Given a weighted automaton, one can build a corresponding linear weighted automaton
by considering the free vector space generated by the set of states $X$ in the WA,
and by linearizing $o$ and $t$. If $X$ is finite, as in our implementations
of the algorithms, we can use the same matrices for 
$t$ and $o$ in both the WA and the corresponding LWA.
We are only considering a finite number of states and therefore finite dimensional
vector spaces. Let $n$ be the number of states in an WA.
We have that in the corresponding LWA, the transition functions $t_a$ are still
 represented as
$\K^{n \times n}$ matrices. $o \in \K^{1 \times n}$ is represented as a row vector.
$t_a(v)$ denotes the vector obtained by multiplying the matrix $t_a$ by the column 
vector $v  \in \K^{n \times 1}$. $o(v)$ denotes the scalar $s \in \K$ obtained by 
dot product of the row vector $o$ with $v \in \K^{n \times 1}$.

\begin{defn}
  The language recognized by a vector $v \in V$ of an LWA $(V,o,t)$ is defined
  for all words $w \in A*$ as $\langlwa{v}{V}(w) = o(v_n)$ where $v_n$ is the 
  vector reached from $v$ through the composition of the transition functions
  corresponding to the words in $w$.
  
  \begin{equation*}
    \begin{aligned}
      \langlwa{v}{V}(w) = \begin{cases}
        o(v) & \text{if } w = \epsilon \\ 
        \langlwa{t_a(v)}{V}(w') & \text{if } w = aw' 
      \end{cases}
    \end{aligned}
  \end{equation*}
  
\end{defn}


We define $\llwb$ as the behavioral equivalence for a given LWA $(V, o, t)$ as 

\begin{equation}
  \forall v_1, v_2 \in V, \; v_1 \llwb v_2 \iff \langlwa{v_1}{V} = \langlwa{v_2}{V}
\end{equation}



Proof is available in section 3.3 of \cite{BONCHI201277}

Language equivalence can be now expressed in terms of linear weighted 
bisimulations (LWBs for short).
Differently from weighted bisimulations, LWBs can be seen both as relations 
and as subspaces.
The subspace representation of LWBs is used in the backwards partition 
refinement algorithm 
implemented in \cite{BONCHI201277} and in this work.

\begin{defn}
  \textit{Linear Relations:}\\
  Let $U$ be a subspace of $V$. The binary relation $R_U$ over $V$ is defined by
  \begin{equation*}
    \begin{aligned}
      v_1 \; R_U \; v_2 \quad \iff \quad v_1 - v_2 \in U
    \end{aligned}
  \end{equation*}
  The relation $R$ is linear if there exists a subspace $U$ such that $R \equiv R_U$.
  A linear relation is a total equivalence relation on $V$.
\end{defn}

\begin{defn}
  \textit{Kernel of a Relation and Linear Extension} \\
  Let $R$ be a binary relation over V. 
  The \textit{kernel} of $R$, is the set 
  $\mker{R} = \left\lbrace v_1 - v_2 \mid v_1 \; R \; v_2 \right\rbrace$.
  The \textit{linear extension} of $R$, written as $R^\ell$, is defined by 
  \begin{equation*}
    \begin{aligned}
      v_1 \; R^\ell \; v_2  \quad \iff \quad (v_1 - v_2) \in \mspan{\mker{R}}
    \end{aligned}
  \end{equation*}
\end{defn}

\begin{lem}
  Let $U$ be a subspace of $V$, then $\mker{R_U} = U$
\end{lem}

\begin{defn}
  \textit{Linear Weighted Bisimulation:} \\
  Let $(V, o, t)$ be a linear weighted automaton. A linear relation 
  $R \subseteq V \times V$ is a \textit{linear weighted bisimulation} if 
  $\forall (v_1, v_2) \in R$ it holds that: 
  \begin{center}
    \begin{enumerate}
      \item $o(v_1) = o(v_2)$
      \item $\forall a \in A, \; t_a(v_1) \; R \; t_a(v_2)$
    \end{enumerate}
  \end{center}
\end{defn}

\begin{lem}
  Let $(V, o, t)$ be a linear weighted automaton. A linear relation 
  $R$ over $V$ is a linear weighted bisimulation if and only if
\end{lem}
  \begin{center}
    \begin{enumerate}
      \item $R \subseteq \mker{o}$
      \item $R$ is $t_a$-invariant $\forall a \in A$
    \end{enumerate}
  \end{center}

Theorem 3 in section 3.3 of \cite{BONCHI201277}, states that 
$\mker{\langlwa{-}{V}}$ is the largest linear weighted bisimulation on $V$.
As a corollary, we obtain that $\llwb$ is the largest linear weighted bisimulation.

We now introduce a lemma that will be fundamental in the next sections of this work.

\begin{lem}
  \label{lem:coincide}
  $\llwb$ coincides with $\sim_l$: 

  Let $(X, o, t)$ be a WA and $(\K^X, o^\sharp, t^\sharp)$ the corresponding linear 
  weighted automaton. Then $\forall x \in X, \;\; \sqmap{x} = \langlwa{x}{\K^X}$
\end{lem}