\section{Algorithms}
The first algorithm we implement to compute language equivalence, called \texttt{HKC},
is adapted from \cite{DBLP:journals/corr/Bonchi0K17}. 
The algorithm returns \texttt{true} $\iff \sqmap{v_1} = \sqmap{v_2}$.
It was first introduced by 
Bonchi and Pous in \cite{bonchi2013checking}.
The algorithm, extending the Hopcroft and Karp procedure 
\cite{hopcroft1971linear} with \textit{congruence closure}, is 
proven to be sound and complete.
It is defined for 
WAs over semirings, but in this implementation we are only 
considering fields, in particular 
the field of real numbers ($\K = \R$).
The problem of checking language equivalence 
has been proven undecidable for an arbitrary semiring, so termination 
may not always be guaranteed. However, it has been shown to be decidable
for a broad range of semirings, for example, all the complete and
distributive lattices.
\texttt{HKC} computes $v_1 \sim_l v_2$ for a given weighted automaton
$W = (X, t, o)$ and two vectors $v_1, v_2 \in \K^X$. 
by computing a congruence closure,
and it does so without linearizing the state space. 

We compare \texttt{HKC} with an algorithm called 
\textit{Backwards Partition Refinement}, that we will call BPR for short. 
Adapted from \cite{BONCHI201277}, 
\texttt{BPR}
is a fixed-point iterative method that, given an LWA
$L = (V, t, o)$, computes a basis of the subspace of $V$
representing the complementary relation of $\llwb$ (we later show it to be the 
orthogonal complement in case $V$ is an inner product space). 
Another version of the algorithm is defined in the same work,
called \textit{Forward Partition Refinement}, which directly computes
a basis for $\llwb$ but is shown to be way less efficient than the backwards version.

Our implementation is directly modeled on the algorithm shown in 
\cite{boreale2009weighted}, since we are fixing weights on $\R$ and computing the orthogonal complements instead of 
dual spaces and annihilators. 

\begin{note}
  Recall from section \ref{sec:notation} that $\llwb$ is a linear relation, 
  therefore $v_1 \llwb v_2 \iff (v_1 - v_2) \in \mker{\llwb}$
\end{note}


From Lemma \ref{lem:coincide}, it follows that given an LWA $L = (V,o,t)$ and a corresponding
basis of $\llwb$ computed by BPR, one can check language equivalence 
of two vectors in the state space, $v_1 \sim_l v_2 $,  by checking if $(v_1 - v_2) \in \mker{\llwb}$.
Therefore, we can say that BPR "\textit{minimizes}", or it computes the whole
binary linear relation $\llwb$, coinciding with $\sim_l$.



The \texttt{BPR} algorithm starts from the basis of a relation $R_0$, that is the complement 
of the relation identifying vectors with equal weights.
It then incrementally computes the space of all states that are reachable from 
$R_0$ in a \textit{backwards} direction. Intuitively, "going backwards" means 
working with the transpose transitions functions $t_a^T$.

\texttt{BPR} has a cost of $O(n^4)$ operations 
to initially compute the largest linear weighted bisimulation,
which can be eventually reduced to $O(n^3)$ \cite{BONCHI201277}.
In our implementation, by initially computing a basis of the orthogonal complement of $\llwb$,
the cost of checking if two vectors are language equivalent is then reduced to the
cost of matrix multiplication ($O(n^2)$). It follows that 
\texttt{BPR} is a great choice when we
have to decide if a large number of vectors in a WA are language equivalent.


In the next sections we will compare execution results of our implementation of the algorithms
\texttt{BPR} and \texttt{HKC} to verify correctness,
and to provide insight on runtime results.
Lemma \ref{lem:coincide}, introduced above, is key to our work. By stating that 
$\llwb$ coincides with $\sim_l$, we can confidently say that the two algorithms 
compute an answer for same the decision problem:

\begin{center}
    Are two vectors $v_1$ and $v_2$ language-equivalent for a given weighted automata? 
\end{center}

%TODO
%TODO costo computazionale HKC.


%=========================================================================

\subsection{HKC Algorithm}
We give the pseudocode definition of the \texttt{HKC} procedure from \cite{DBLP:journals/corr/Bonchi0K17}:

\captionof{figure}{The \texttt{HKC}($v_1, v_2$) procedure}
\lstinputlisting[escapeinside={(*}{*)}]{hkc.txt}
\label{fig:hkc}

\subsection{Backwards Partition Refinement Algorithm for the Largest Weighted Bisimulation}
\label{sec:algo2}

We now recall the backwards algorithm for computing $\llwb$ defined in \cite{BONCHI201277}.
The algorithm is defined by the iterative method:
\begin{eqnarray}
  R_0 = \mker{o}^0, & \quad & R_{i+1} = R_i + \sum_{a \in A} t_a^T(R_i) \label{back} 
\end{eqnarray}
Where $\mker{o}^0$ is an annihilator.

\begin{note}
  Given two vector spaces $V_1, V_2$ we write $V_1 + V_2$ to 
  denote $\mspan{V_1 \cup V_2}$
\end{note}


The algorithm stops when $R_{j+1} = R_j$. An index $j \leq \dim(V)$ is 
guaranteed to exist, such that the algorithms stops at step $j$.
It follows that $\llwb = R_j^0$.
Proof is available in section 4.2 of \cite{BONCHI201277}
