\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[table]{xcolor}
\usepackage{stmaryrd}
\usepackage{amsfonts}
\usepackage{lstlistings-golang}

% Code with syntax highlighting
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{placeins}


\theoremstyle{plain}% default
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{exrc}[exmp]{Exercise}
\theoremstyle{remark}
\newtheorem*{comm}{Comment}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}

\def\C{\mathbb{C}}
\def\N{\mathbb{N}}
\def\Q{\mathbb{Q}}
\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\K{\mathbb{K}}

\DeclarePairedDelimiter\sqmap{\llbracket}{\rrbracket}
\DeclarePairedDelimiter\spanset{\langle}{\rangle}


\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\mspan}[1]{\text{span}\left(#1\right)}
\newcommand{\dom}{\text{dom}}
\newcommand{\mker}[1]{\text{ker}\left(#1\right)}
\newcommand{\mrank}[1]{\text{rank}\left(#1\right)}
\newcommand{\llwb}{\approx_{\mathcal{L}}}
\newcommand{\langlwa}[2]{\sqmap{#1}^\mathcal{L}_{#2}}

% matrix commands
\newcommand{\vvec}[2]{
	\begin{pmatrix}
		#1 \\ #2
	\end{pmatrix}
}
\newcommand{\vvvec}[3]{
	\begin{pmatrix}
		#1 \\ #2 \\ #3
	\end{pmatrix}
}
\newcommand{\vvvvec}[4]{
	\begin{pmatrix}
		#1 \\ #2 \\ #3 \\ #4
	\end{pmatrix}
}

% Code Listings
\definecolor{vgreen}{RGB}{104,180,104}
\definecolor{vblue}{RGB}{49,49,255}
\definecolor{vorange}{RGB}{255,143,102}
\lstset{
	basicstyle=\ttfamily,
	columns=fullflexible,
	keepspaces=true,
}

\title{Go Implementation of Up-To Techniques for Equivalence of Weighted Languages}

%\thanks{Use footnote for providing further
%information about author (webpage, alternative
%address)---\emph{not} for acknowledging funding agencies.}

\author{
  Alessandro Cheli \\
  Undergraduate Student \\
  Department of Computer Science \\
  Universit√† di Pisa\\
  Pisa, PI 56127 \\
  \texttt{a.cheli6@studenti.unipi.it} \\
  %% examples of more authors
  %% \And
  %% Elias D.~Striatum \\
  %% Department of Electrical Engineering\\
  %% Mount-Sheikh University\\
  %% Santa Narimana, Levand \\
  %% \texttt{stariate@ee.mount-sheikh.edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
Weighted automata generalize non-deterministic automata by adding
a quantity expressing the weight (or probability) of the execution of each transition.
In this work we propose an implementation of two algorithms for computing language 
equivalence in finite state weighted automata (WAs). The first, a
linear partition refinement algorithm, calculates the largest linear weighted bisimulation
for any given LWA (Linear Weighted Automaton) through an iterative method, 
the second algorithm checks the language equivalence 
of two vectors (states) for a given weighted automata by using an additional
data structure representing a congruence relation between states.
We then compare results of the two algorithms to verify their correctness
and performance on randomly generated samples. 
We finally provide comparison of runtime statistics and suggest 
which of the two algorithm is a the best choice for some usage cases.
\end{abstract}


% keywords can be removed
\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
\label{sec:intro}

In \cite{DBLP:journals/corr/Bonchi0K17}, up-to techniques are defined for
weighted systems over arbitrary semirings, while in \cite{BONCHI201277}, up-to techniques
are defined for Linear Weighted Automata (LWAs), under a more abstract coalgebraic perspective.

\section{Preliminaries and Notation}
\label{sec:notation}

\begin{note}
  Given two vector spaces $V_1, V_2$ we write $V_1 + V_2$ to denote $\mspan{V_1 \cup V_2}$
\end{note}


\begin{defn}
  A \textit{weighted automaton} over a field $\K$ and an alphabet $A$ is a triple 
  $(X,o,t)$ such that $X$ is a finite set of states, 
  $t = \left\lbrace t_a : X \to \K\right\rbrace_{a \in A}$
  is a set of transition functions indexed over the symbols of the alphabet $A$ and $o : X 
  \to \K$ is the output function. 
  $A^*$ is the set of all words over $A$. $\epsilon$ is the empty word and $aw$ is the
  concatenation of a symbol $a$ to the word $w \in A^*$.
  A weighted language is a function $\psi: A^* \to \K$.
  A function mapping each state vector into its 
  accepted language, $\sqmap{\cdot}: \K^X \to \K^{A^*}$ is defined as follows for every weighted automaton:

  \begin{equation*}
    \begin{aligned}
      \forall v \in \K^X, a \in A, w \in A^* \quad \quad
      \sqmap{v}(\epsilon) = o(v) \quad \quad
      \sqmap{v}(aw) = \sqmap{t_a(v)}(w)  
    \end{aligned}
  \end{equation*}
\end{defn}

Two vectors $v_1, v_2 \in \K^{X\times 1}$ are weighted called language equivalent, denoted with 
$v_1 \sim_l v_2 $ if and only if $ \sqmap
{v_1} = \sqmap{v_2}$. One can extend the notion of language equivalence to states rather
than for vectors by assigning to each state $x \in X$ the corresponding unit vector 
$e_x \in \K^X$. When given an initial state $i$ for a weighted automaton, the language 
of the automaton can be defined as $\sqmap{i}$.


\begin{defn}
  A binary relation $R \subseteq X \times Y$ between two sets $X, Y$ is a subset of the 
  cartesian product of the sets. A relation is called \textit{homogeneous} or an \textit
  {endorelation} if it is a binary relation over $X$ and itself: $R \subseteq X \times
   X$. 
  In such case, it is simply called a binary relation over $X$.
  An \textit{equivalence relation} is a binary relation that is reflexive, symmetric and
  transitive. 
\end{defn}

\begin{defn}
  The congruence closure $c(R)$ of a relation R is the smallest congruence relation 
  $R'$ such that $R \subseteq R'$ 
\end{defn}

An equivalence relation which is compatible with all the operations of
the algebraic structure on which it is defined on, is called a 
\textit{congruence relation}. Compatibility with the algebraic structure operations
means that algebraic operations applied on equivalent elements will still
yield equivalent elements. 



%===================================================================================

We omit the coalgebraic definition for \textit{linear weighted automata} seen in 
\cite{BONCHI201277} and give a more intuitive definition. 
In this implementation, we focus only on weighted automata defined over 
the field of real numbers $\R$. 

\begin{defn}
  A \textit{linear weighted automaton} (in short, LWA) over the field $\K$ and an alphabet $A$
  is a triple  $L = (V, o, \left\lbrace t_a \right\rbrace_{a \in A})$ 
  where $V$ is a vector space representing the state space, 
  $o: V \to \K$ is a linear map associating to each state its output weight,
  and $t = \left\lbrace t_a = V \times V \right\rbrace_{a \in A}$ is
  the set of transition functions, represented with liner maps 
  that for each input $a \in A$ associate the next state, in this case a vector
  in $V$.
  As in \cite{boreale2009weighted}, we have that $\dim{(L)} = \dim{(V)}$.
\end{defn}

Given a weighted automaton, one can build an isomorphic linear weighted automaton
by considering the free vector space generated by the set of states $X$ in the WA,
and by linearizing $o$ and $t$. If $X$ is finite, we can use the same matrices for 
$t$ and $o$ in both the WA and the corresponding LWA.
We are only considering a finite number of states and therefore finite dimensional
vector spaces. Let $n$ be the (finite) number of states in an WA.
We have that in the corresponding LWA, the transition functions $t_a$ are still
 represented as
$\K^{n \times n}$ matrices. $o \in \K^{1 \times n}$ is represented as a row vector.
$t_a(v)$ denotes the vector obtained by multiplying the matrix $t_a$ by the column 
vector $v  \in \K^{n \times 1}$. $o(v)$ denotes the scalar $s \in \K$ obtained by 
dot product of the row vector $o$ with $v \in \K^{n \times 1}$.

\begin{defn}
  The language recognized by a vector $v \in V$ of an LWA $(V,o,t)$ is defined
  for all words $w \in A*$ as $\langlwa{v}{V}(w) = o(v_n)$ where $v_n$ is the 
  vector reached from $v$ through the composition of the transition functions
  corresponding to the words in $w$.
  
  \begin{equation*}
    \begin{aligned}
      \langlwa{v}{V}(w) = \begin{cases}
        o(v) & \text{if } w = \epsilon \\ 
        \langlwa{t_a(v)}{V}(w') & \text{if } w = aw' 
      \end{cases}
    \end{aligned}
  \end{equation*}
  
\end{defn}


We define $\llwb$ as the behavioral equivalence for a given LWA $(V, o, t)$ as 

\begin{equation}
  \forall v_1, v_2 \in V, \; v_1 \llwb v_2 \iff \langlwa{v_1}{V} = \langlwa{v_2}{V}
\end{equation}


\begin{lem}
  $\llwb$ coincides with $\sim_l$: 

  Let $(X, o, t)$ be a WA and $(\K^X, o^\sharp, t^\sharp)$ the corresponding linear 
  weighted automaton. Then $\forall x \in X, \;\; \sqmap{x} = \langlwa{x}{\K^X}$
\end{lem}

\begin{proof}
  Proved in section 3.2 of \cite{BONCHI201277}
\end{proof}


%=========================================================================





\section{The Problem}
In this work, we  


\section{Algorithms}

We now recall the backwards algorithm for computing $\llwb$ defined in \cite{BONCHI201277}.

\subsection{Backwards Partition Refinement Algorithm for the Largest Weighted Bisimulation}
\label{sec:algo2}

The algorithm is defined by the iterative method:
\begin{eqnarray}
  R_0 = \mker{o}^0, & \quad & R_{i+1} = R_i + \sum_{a \in A} t_a^t(R_i) \label{back} 
\end{eqnarray}
Where $\mker{o}^0$ is an annihilator.
The algorithm stops when $R_{j+1} = R_j$. Proof is available in section 4.2 of \cite{BONCHI201277}



%\subsection{Forward Partition Refinement Algorithm for the Largest Weighted Bisimulation}
%\label{sec:algo2}
%
%The algorithm is defined by the iterative method:
%\begin{eqnarray}
%  R_0 = \mker{o}, & \quad & R_{i+1} = R_i \cap \bigcap_{a \in A} t_a(R_i)^{-1} \label{fwd} 
%\end{eqnarray}
%The algorithm stops when $R_{j+1} = R_j$. Proof is available in section 4.1 of \cite{BONCHI201277}

\section{Implementation}
\label{sec:impl}

The algorithms and data structures for this paper are implemented in the Go programming 
language. This implementation makes use of the Gonum library for numerical 
computations. We only import the Gonum libraries for matrices and linear algebra 
and visual plotting of samples and functions.
Real numbers are implemented with double precision floating point numbers,  
known as the \texttt{float64} type in the Go programming language.




See Section \ref{sec:headings}.



\begin{defn}
  \textbf{Algorithm for computing the null space of a vector subspace} \\
  The algorithm implementation can be found in file \texttt{lin/nullspace.go}; it is
  adapted from \cite{scipy/ranknullspace}. 
  Let's consider the singular value decomposition of a matrix $A \in \R^{m \times n}$:

  \begin{equation*}
    \begin{aligned}
      A = U \Sigma V^T & \quad & \Sigma = \diag{\sigma_1, \sigma_2, \hdots, \sigma_{\min(m, n)}  } 
       & \quad &  U \in \R^{m \times m} & \quad & V \in \R^{n \times n}
    \end{aligned}
  \end{equation*}

  Where $V$ and $U$ are orthogonal and the singular values are ordered: $\sigma_1 \geq \sigma_2 \geq \hdots \geq \sigma_{\min(m,n)} \geq 0$.
  It follows that $\mrank{A}$ is equal to the number of nonzero singular values and
  a basis of the (right) null space of $A$ is the spanning set of the columns of V
  corresponding to singular values equal to zero. 
\end{defn}

\begin{exmp}
  First, we show a shorter Python 
  implementation of the algorithm to compute the nullspace, using the 
  \textit{SciPy} library \cite{scipy/ranknullspace}:

  \lstinputlisting[language=python]{nullspace.py}

    The Go implementation is quite longer:

    \lstinputlisting[language=go]{../lin/nullspace.go}

\end{exmp}



\subsection{Implementing the backwards algorithm}

In \cite{BONCHI201277}, for the first step of the iterative backwards 
algorithm to compute the largest linear weighted bisimulation, we 
need to compute $\mker{o}^0$. If $V$ is a vector space and $W$ is a
subspace of $W$, the annihilator of $W$, respectively $W^0$ is 
a subspace of the space $V^*$ of linear functionals on $V$.
$W^0$ are the functionals that annihilate on $W$. Since 
we are working on subspaces of $\R^n$, we can directly compute 
the orthogonal complement in our implementation instead of the
annihilator.


\begin{prop}
  If $V$ is a finite dimensional vector space defined with an inner product
  $\langle \cdot , \cdot \rangle$ and $W$ is a subspace of $V$
  then the image of the annhilitaor $W^0$ through the linear 
  isomorphism $\varphi: V^* \to V$ induced by the inner product, 
  is the orthogonal of $W$ with respect to the said inner product.
\end{prop}

\begin{proof}
  Let $V$ be an inner product space over the field $\K$ with an inner product defined as
  $\langle \cdot , \cdot \rangle : V \times V \to \K$. 
  Every linear functional can be 
  represented with a vector. Let $\xi : V \to \K$ be a functional, 
  $\xi \in  W^0$. Because $\xi(w)=0 \quad \forall  w \in W$, 
  if $v$ represents $\xi$ we have that $(v, w)=\xi(w)=0$ for all $w \in W$. 
  We obtain that $\varphi(W^0) \subseteq W^{\perp}$.
  If $v \in W^\perp$  
  then the functional $x \mapsto (v, x)$ cancels over $W$ 
  (by the definition of orthogonality).
\end{proof}


To compute the orthogonal complement of a vector subspace $W$, we
compute $W^\perp = \mker{A^T}$, where $A$ is the matrix with 
column vectors in the spanning set of $W$ as its columns. Precisely, $W$ is 
represented as the 
column space of $A$. Proof is available in \cite{ila}.



\subsubsection{Headings: third level}

\paragraph{Paragraph}




\section{Examples of citations, figures, tables, references}
\label{sec:others}

\cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}





\bibliographystyle{unsrt}
\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.

% 
% %%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}
% 
% \bibitem{kour2014real}
% George Kour and Raid Saabne.
% \newblock Real-time segmentation of on-line handwritten arabic script.
% \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
%   International Conference on}, pages 417--422. IEEE, 2014.
% 
% \bibitem{kour2014fast}
% George Kour and Raid Saabne.
% \newblock Fast classification of handwritten on-line arabic characters.
% \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
%   International Conference of}, pages 312--318. IEEE, 2014.
% 
% \bibitem{hadash2018estimate}
% Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
%   Jacovi.
% \newblock Estimate and replace: A novel approach to integrating deep neural
%   networks with existing applications.
% \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.
% 
% \end{thebibliography}
% 
% 
\end{document}
 